{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vWSjvSv94GOZ"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from keras.models import model_from_json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CXWcsrGW4gJt"},"outputs":[],"source":["emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n","\n","# load json and create model\n","json_file = open('/content/drive/MyDrive/Human Activity Recognition/Emotion_detection_with_CNN-main/model/emotion_model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","emotion_model = model_from_json(loaded_model_json)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1108,"status":"ok","timestamp":1702365434495,"user":{"displayName":"Yatharth Singh","userId":"02529296618263272670"},"user_tz":-330},"id":"upr2_Az94qhb","outputId":"d6bee811-f990-4e2c-d01c-fae00e2bf8f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded model from disk\n"]}],"source":["# load weights into new model\n","emotion_model.load_weights(\"/content/drive/MyDrive/Human Activity Recognition/Emotion_detection_with_CNN-main/model/emotion_model.h5\")\n","print(\"Loaded model from disk\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1wWodRdS5cAS"},"outputs":[],"source":["from google.colab import output\n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","\n","\n","# # Install required packages (if not already installed)\n","# !pip install opencv-python-headless\n","# !pip install numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"QoBAET8d5ApM","outputId":"9ddf63fa-1401-4bf8-a893-41b232d64f27"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","\n","# Pass the video path\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/Human Activity Recognition/Emotion_detection_with_CNN-main/Emotion.mov\")\n","\n","# Get the input video dimensions\n","width = int(cap.get(3))\n","height = int(cap.get(4))\n","\n","# Set the output video file path\n","output_path = \"/content/drive/MyDrive/Human Activity Recognition/Emotion_detection_with_CNN-main/output_video.mp4\"\n","\n","# Define the codec and create a VideoWriter object with the same dimensions as the input video\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))  # Use the dimensions of the input video\n","\n","while True:\n","    # Find haar cascade to draw bounding box around face\n","    ret, frame = cap.read()\n","    if not ret:\n","        break  # Break out of the loop when there are no more frames\n","\n","    frame = cv2.resize(frame, (width, height))\n","    face_detector = cv2.CascadeClassifier('/content/drive/MyDrive/Human Activity Recognition/Emotion_detection_with_CNN-main/haarcascades/haarcascade_frontalface_default.xml')\n","    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # detect faces available on camera\n","    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n","\n","    # take each face available on the camera and preprocess it\n","    for (x, y, w, h) in num_faces:\n","        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n","        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n","        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n","\n","        # predict the emotions\n","        emotion_prediction = emotion_model.predict(cropped_img)\n","        maxindex = int(np.argmax(emotion_prediction))\n","        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n","\n","    # Display the frame\n","    cv2_imshow(frame)\n","\n","    # Write the frame to the output video\n","    out.write(frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","# Release the VideoCapture and VideoWriter objects\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWRmiR-d50tp"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1RmY6cek2gpUWOo1NRHOR8XAOR_HjMIjC","authorship_tag":"ABX9TyM9AM1th9phTYwm3EhsET18"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}